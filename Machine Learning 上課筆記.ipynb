{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "周凡剛 Elwing birdfan8814@gmail.com\n",
    "\n",
    "機器學習\n",
    "有趣的 AI 範例: github style2paints , starGAN , edge cat\n",
    "AI ( 終極目標 ) : 包含 真理(公式) + 經驗(機器學習)\n",
    "深度學習 : 利用類神經系統進行機器學習無法處理之抽象物件( 文字 , 影像 ) \n",
    "\n",
    "環境安裝 python3.7 + pyth\n",
    "jupyter scikit-learn pandas matplotlib seaborn\n",
    "\n",
    "2019-jupyter 麼開\n",
    "抓 pythoen37.exe and jupyter.exe 的資料夾路徑 => 加入 ==> 本機內容 - 環境變數\n",
    "\n",
    "函式庫\n",
    "機器學習 ( Scikit Learn) => 深度學習 ( Tenson flow , Karas ), Tenson flow 包裝為 Karas \n",
    "\n",
    "環境安裝\n",
    "Notebook\n",
    "Scikit Learn (機器)\n",
    "Pandas (表格)\n",
    "matplolibt (畫圖)\n",
    "seaborn\n",
    "\n",
    ".[sklearn練習網站](https://scikit-learn.org/stable/datasets/index.html)\n",
    "\n",
    "step1. 準備資料 (不須篩選資料相關性)\n",
    "資料分為: \n",
    "(1). 有答案: 監督式學習  =>\ta. 無大小關係--分類(Classification)  \n",
    "b. 有大小關係--回歸(Regression)\n",
    "(2). 沒答案: 非監督式學習 => 無大小關係--分群(Clustering)\n",
    "(3).答案自動由環境產出: 半監督式(自動車, 機器人Game) \n",
    "=> 強化學習(Reinforcement learning)\n",
    "(4)資料處理盡量存成csv\n",
    "    csv: (Common-Seperateed Values) => 1. ,=隔開, 2. [enter] = 筆, 3. 資料中有,怎麼辦 => \"資料中有,怎麼辦\"\n",
    "\n",
    "step2. 建立訓練模型\n",
    "\n",
    "決策樹 : 以特徵(問題)分別出目標(答案)\n",
    "安裝 graphviz :\n",
    "windows => 安裝 graphviz : 輔助劃出決策樹\n",
    "mac => 安裝  mac homebrew : 套件管理 => brew graphviz \n",
    "新增 path 的環境變數 加入 dot.exe 的路徑 C:\\Program Files (x86)\\Graphviz2.38\\bin\n",
    "\n",
    "ClassiFicasionTree : 分類( accuracy score ) , 選擇題 , \n",
    "資料集 : scikit-learn dataset 用 pandas 處理\n",
    "鳶尾花 : 監督式 + 分類\n",
    "data 每個特徵 v.s. Target 相關性\n",
    "模型驗證: 90% 做模型訓練, 10%驗證\n",
    "\n",
    "流程 : \n",
    "1. 匯入資料集\n",
    "2. 用pandas處理資料為表格\n",
    "3. 資料具現化: 使用 matplotlib , seaborn , 用matplotlib colormap 選顏色\n",
    "4. 匯入分類模式 : 以 train_test_split 分類, df 代入資料, 將資料分類成 x_train, x_test....\n",
    "5. 匯入決策樹 : 匯入 DecisionTreeClassifier, 代入分類後的資料 clf(x_train, y_train)\n",
    "6. 畫出決策樹 : 匯入export_graphviz 製作決策樹, 匯入 graphviz(決策樹) 畫出\n",
    "7. 結果驗證 : 以 predict 驗證結果,為方便比較可以 list 表示\n",
    "8. 預測成功機率評估 : 匯入 accuracy_score , 代入比較資料 (y_test, pre)\n",
    "9. 混淆舉證抓錯誤項目 : 匯入並代入 confusion_matrix(y_test, pre),再以pandas表示\n",
    "** 重點 **  sklearn 用法 : 創建 clf → clf.fit(x_train, y_train) → pre = clf.predict(x_test)\n",
    "\n",
    "亂度 E (Entropy): log2 1/P =>  最亂 : 1 , 最不亂 : 0 , 往亂度低的向移動\n",
    "增益 : (old E) - (new E) => 增益最大 : 1 , 最小 : 0, \n",
    "gini 係數 : 照機率猜的平均錯誤機率, 純粹程度 增益的相反, => 最純粹 0 , 最不純粹 1 , 往越純粹的向移動\n",
    "    ex.: 一共有70工程師,30個非工程師, gini 選工程師和選非工程師的錯誤機率相加(70%*30%+30%*70%)\n",
    "聲音的 分貝,頻率 => 都是取 Log 做運算\n",
    "    \n",
    "Regression : 連續( r2_score ) ,計算題\n",
    "系統 : 穩定 - 巨觀 , 亂度高 - 微觀\n",
    "決策 : 欠擬合 ( 不準 ) => 擬合 ( 準 ) => 過擬合 ( 不準 ) , 應盡量避免過擬合( depth 太深 )\n",
    "避免過擬合方法 : 1. 前剪枝 : 決定 max_depth , 2. 後剪枝 : 試著剪, 不行再加回來, sklearn 不支援\n",
    "depth 決定 : 經驗科學, 窮舉法, \n",
    "演算法選擇 : 1. 決策樹 - 可解釋性強=>說服力強 - 太依賴經驗法則\n",
    "AI 三種題型 : 1.  選擇題, 2. 計算題, 3. 沒答案\n",
    "\n",
    "mse : 集中的程度,  1/n*(x-平均)^2\n",
    "RMSE : \n",
    "MAE : \n",
    "/fi󠀲 = 所有測試資料值平均\n",
    "fi = 第 i 筆測試資料的值 (real)\n",
    "pi = 地 i 筆測試資料預測值\n",
    "b = ( /fi - fi )^2 => 第 i 筆資料真實值與平均值的差平方\n",
    "a = (pi - fi )^2 => 第 i 筆資料真實值與預測值的差平方\n",
    "特別值: \n",
    "a / b : = 0 => a = 0 , 所有 pi = fi , 超級準 , 很像分類的100%\n",
    "          = 1 => a = b , 所有 pi = fi , 不看提值些瞎猜平均數 , 很像分類的50%\n",
    "r2 score : 1(準) ~ 1 - a/b ~0(瞎) , 分類 : 用80%以上 , 迴歸 : 用0.5以上\n",
    "整理 : \n",
    "\t1. 選擇題 : Iris (Classification)\n",
    "\t    答案 : 類別 ( accuracy score ) , 75~80 (OK) , 80~90(好)\n",
    "\t2. 計算題 : Boston ( Regression )\n",
    "\t    答案 : 連續 ( r2 score ) , 0.5~0.7 (OK) , 0.7~1.0 (好)\n",
    "\t3. 分群 : KMeans ….. k = ? , 鳶尾花 ...... k = 3\n",
    "\n",
    "分群步驟 : \n",
    "1. 任選3點為圓心 => 2. 剩餘點以遠近分3群 = 修正直到圓心不再改變 => 3.取這三群的圓心\n",
    "KMeans++ : 一開始三點選分開一點\n",
    "分群找圓心:\n",
    "1. 親近程度: a= EE(pi-所在圈圈的其他點)^2 , 所有圈圈內的所有點跟所在圈圈內的其他點的距離\n",
    "2. 疏遠程度: b= EE(pi-最近圈圈的其他點)^2 , 所有圈圈內的所有點跟最近圈圈的其他點的距離\n",
    "好的分群 : 跟自己圈內得很近( 距離為 a ) , 跟圈外的人遠 ( 距離為 b ) => a <= b => a/b 越小越好 , \n",
    "特殊值: a/b = {0 : a->0 跟自己圈得很近, b->無限大 跟其他圈的人超疏遠 , 1 : a=b 牆頭草(瞎分, 同 r2_score)}\n",
    "Silhonette score(親疏程度): {1 - a/b = 0:最爛 , 1:最好}\n",
    "未知 K : 利用 Sihouette Score 對 K 作圖 => 找出 Sihoette Score 最大時對應的 KMeans 值\n",
    "    利用:seaborn.scatterplot(x,y,hue(分群的顏色))\n",
    "\n",
    "流程 : \n",
    "匯入資料集並利用 pandas 製作沒有答案的表格\n",
    "匯入分類模式 : 以 train_test_split 分類, df 代入資料, 將資料分類成 x_train, y_test....\n",
    "匯入 KMeans 代入測試資料做公式並 fit\n",
    "利用 clu.labels_ 產生 array\n",
    "結果驗證 : 以 predict 搭配 list 列出預測結果 , 匯入 accuracy_score 算正確率\n",
    "匯入 silhouette_score 搭配迴圈 , 求出 k 值及 silhouette_score\n",
    "匯入 matplotlib.pyplot 以 plt.plot(plotx, ploty, \"co--\") 畫圖\n",
    "\n",
    "維度災難: 欄位太多 => 1. 一般演算法無法處理, 2. 資料要準備更多更多(一維:10筆,二維:100筆,三維:1000筆,....)\n",
    "簡單貝式原理 : 假設所有機率皆為獨立事件, 以骰骰子為假設, 不真實, 但可以比大小\n",
    "p(A) : A 發生的機率 , p(A/B) : 固定 B 的條件下 A 發生的機率\n",
    "公式 : 連續 => 相乘 , 並行 => 相加\n",
    "p(A)*p(B/A) = p(B)*p(A/B) ==> p(B/A) = [ p(B) / p(A) ] * p(A/B)\n",
    "獨立事件: 這次機率 不會 影響下次的機率, 連續骰骰子都是 1 的機率, p(A)*p(B)\n",
    "非獨立事件: 這次機率 會 影響下次的機率, 連續兩天下雨機率, p(A)*p(B/A)\n",
    "\n",
    "單純貝式種類(Naive Bayes) :\n",
    "GaussianNB : (連續特徵) 常態分布,高斯分布 , 不常用 iris(還是用決策樹比較好)\n",
    "MultionmialNB : (整數特徵) 擲骰子, 詩詞\n",
    "BernoulliNB : 兩面骰( 是 & 否 ) , Multionmia 特化\n",
    "\n",
    "jieba : 重要性 = 次數 * log(1/常用)\n",
    "\n",
    "詩詞判斷流程 : \n",
    "1. jieba 分詞 ( testdf ) : pandas 處理資料成表格 + jieba cut 對每一句分詞 + apply 到整首詩\n",
    "2. scikit-learn ( trans ): 答案一定要整數 => 作者 ==>  [0, 1, 2] \n",
    "3. Vectorizer ( trainvec , testvec) : 匯入CountVectorizer 做字詞統計 \n",
    "(1). fit : 欄位確定, 統計 多少種 標籤 , (2). transform : 轉換, 統計 次數\n",
    "訓練(poem-train) : fit - transform\n",
    "測試(poem-test) : transform , 測試看到沒出現過的就丟掉 => 測試資料不做 fit\n",
    "4. 做公式 clf : 匯入 MultinomialNB 並 fit\n",
    "5. 驗證模型 : 匯入 accuracy_score 並做預測 predict 代入 pre , testdf[\"作者\"]\n",
    "6. 預測 app : 做 rtrans {0:\"李白\"} , 以 input 收詞句 再 分詞以 list 表示,\n",
    "\t做字詞統計 vec.transform , 進行預測 predict , 最後回推 作者\n",
    "\n",
    "1. Tf-idf = 出現次數 / 常用次數 , 用於預先篩選常用字, 但不要用 , 沒差\n",
    "    => 絕對不要預先篩選特徵值 , 應把所有的詞都收錄 , 才不會過於偏頗\n",
    "2. alpha = 1.0 (1.0 的 laplace smoothy ) , 避免機率出現 0 \n",
    "\n",
    "新聞分詞:\n",
    "1. 處理資料:運用 glob.glob(\"路徑\") 創建所有要處理的副檔名字典=>用pandas處里成表格\n",
    "2. 分詞: 匯入jieba字典,將jieba做成方法apply到news裡\n",
    "3. fit_transform :匯入CountVectorizer 分別對分詞完的train和test 的 news進行fit_transform(test不用fit)\n",
    "4. 製作考古題: 匯入MultinomialNB => clf.fit(x_train, y_train)\n",
    "5. 預測: 匯入accuracy_score => clf.predict(x_test) => accuracy_score(pre, y_test)\n",
    "6. 輸出答案(測試): 運用混亂矩陣 confusion_matrix 輸出預測的答案\n",
    "7. 輸入新聞:\n",
    "\n",
    "Kaggles Titanis\n",
    "資料預處理\n",
    "1. 補缺失值 : \n",
    "判斷哪些為數值那些為類別:\n",
    "大小(數值) : 補中位數 , isna() 找缺失 + 找中位數 median() + fillna() 填入 , 絕對不補平均值 , \n",
    "類別(類別) : 找最常出現的字 .value_counts().idxmax()\n",
    "2. 轉換 : 函式庫只吃數字, 類別+大小關係 => 一個選擇題變n個是非題(One-Hot Encoding)\n",
    "\t(1). pd.get_dummies(traindf[\"Embarked\"])  , 轉換\n",
    "\t(2). pd.concat([testdf, dummy], axis = 1) , 接上 , axis=0 直的接, axis=1 橫的接\n",
    "3. 處理比較難的: \n",
    "運用 split + replace 取出我們要的(mid) , 再用 pd.crosstab().T 確認對答案的影響,刪除稀少且平均的項目, 建立方法(nameflow) apply到表格, 再用pd.get_dummies + contact 接回表格\n",
    "4. 資料清理 : 先用 traindf.columns 查目前欄位 , 再將不需要的欄位 drop 掉\n",
    "\n",
    "繪製 heatmap : 不要亂畫 , 先判斷 重要 順序\n",
    "\t如何得知大樣貌 : 1. 決策樹 , 2. 相關係數( -1 <=  0 <=1)\n",
    "\t匯入 matplotlib.pyplot 跟 seaborn, 用 seaborn 畫出:\n",
    "\tplt.figure(figsize=(14,14))\n",
    "sns.heatmap(traindf.corr(), annot = True, cmap=\"RdBu\")\n",
    "\n",
    "Esample(組合) : 平行組合(Bagging) & 連續組合(Boosting)\n",
    "連續組合 : 昨天做的今天不能錯, 公式針對連續執行做調整以優化分類器\n",
    "平行組合 : 三個不同的臭皮匠勝過諸葛亮,想辦法讓模型不同\n",
    "前提 = 每個模型略有不同 => 隨機森林(90% 隨機放棄資料)\n",
    "交叉驗證: 分十份輪流驗證 cross_val_score \n",
    "找最佳參數: 用 GridSearchCV ,best_estimater 可以照最佳參數\n",
    "步驟:\n",
    "1. 匯入 RandomForestClassifier , 代入 max_depth (決策樹幾層)及 n_estimators (幾棵樹)\n",
    "2. 匯入 cross_val_score 輪流驗證模型\n",
    "3. 匯入 GridSearchCV 交叉驗證模型搭配 字典, 確認最好的組合\n",
    "4. 將結果併成 字典 result 儲存上傳至 kaggle\n",
    "提高正確率: 1.資料處理, 2.演算法\n",
    "資料處理:\n",
    "    處理離群值: Robusterscal: sklearn.preprocessing.MinMaxScaler\n",
    "演算法選擇:\n",
    "    KNN(k-nearest neighbors):\n",
    "    一個演算法有距離相關一定要scalling => KMeans\n",
    "    1. 省時間, 2. 需要資料少\n",
    "    RandomForestClassifier:\n",
    "    2. 慢慢試, 2. 資料多, 3. 可以得到特徵重要性(feature_importance)\n",
    "\n",
    "\n",
    "機器學習彙整 : \n",
    "目的 : 求出公式  =>   input -> 公式 ? -> output\n",
    "監督式:\n",
    "選擇題(Classification) : DecisionTree , Naive Bayes (文字) , RandomForest (非文字)\n",
    "計算題(Regression) : DecisionTree -> RandomForest \n",
    "非監督式 : 無標註(Cluster)\n",
    "KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
